{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'active_catinous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-48f2c4685714>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mactive_catinous\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'active_catinous'"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import pytorch_lightning as pl\n",
    "import sklearn \n",
    "from sklearn.metrics import confusion_matrix, auc, roc_curve\n",
    "import torch\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import pickle\n",
    "from py_jotools import slurm, cache\n",
    "import numpy as np\n",
    "import gc\n",
    "#from evalscript import eval_testset, eval_forbwtfwt\n",
    "\n",
    "import hashlib\n",
    "import dill\n",
    "import active_catinous.utils as cutils\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from models.AgePredictor import EncoderRegressor\n",
    "from models.unet3d import EncoderModelGenesis\n",
    "\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import matplotlib\n",
    "from glob import glob\n",
    "import datetime \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "%load_ext autoreload\n",
    "%autoreload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap_model_seeds(hparams, seed=2314134, split='test'):\n",
    "    recalls = {'ges': [], 'geb': [], 'sie': []}\n",
    "    precision = {'ges': [], 'geb': [], 'sie': []}\n",
    "    \n",
    "    hparams['seed'] = seed\n",
    "\n",
    "    model, logs, df_mem, expname = lungmodel.trained_model(hparams, train=False)\n",
    "    model.eval()\n",
    "\n",
    "    for res in ['ges', 'geb', 'sie']:\n",
    "        ds_test = LIDCDataset('/project/catinous/lunadata/luna_lunacombined_dataset_malignancy.csv', \n",
    "                      cropped_to=(288, 288), split=[split], res=res, validation=True)\n",
    "\n",
    "        device = torch.device('cuda')\n",
    "        iou_thres = 0.5\n",
    "\n",
    "        overall_true_pos = dict()\n",
    "        overall_false_pos = dict()\n",
    "        overall_false_neg = dict()\n",
    "        overall_boxes_count = dict()\n",
    "        for k in np.arange(0.0, 1.01, 0.05):\n",
    "            overall_true_pos[k] = 0\n",
    "            overall_false_pos[k] = 0\n",
    "            overall_false_neg[k] = 0\n",
    "            overall_boxes_count[k] = 0\n",
    "            \n",
    "\n",
    "        for batch in ds_test:\n",
    "            img_batch, annot, res, image = batch\n",
    "            img_batch = img_batch.to(device)\n",
    "\n",
    "            out = model.model(img_batch)\n",
    "            out_boxes = [cutils.filter_boxes_area(out[i]['boxes'].cpu().detach().numpy(), out[i]['scores'].cpu().detach().numpy()) for i in range(len(out))]\n",
    "            boxes_np = [b[0] for b in out_boxes]\n",
    "            scores_np = [b[1] for b in out_boxes]\n",
    "\n",
    "            final_boxes, final_scores = cutils.correct_boxes(boxes_np, scores_np)\n",
    "\n",
    "            gt = annot['boxes'][0]\n",
    "            for k in np.arange(0.0, 1.01, 0.05):\n",
    "                false_positives = 0\n",
    "                false_negatives = 0\n",
    "                true_positives = 0\n",
    "                detected = False\n",
    "                boxes_count = 0\n",
    "                if len(final_boxes)>0:\n",
    "                    for i, b in enumerate(final_boxes):\n",
    "                        if final_scores[i]>k:\n",
    "                            boxes_count += 1\n",
    "                            if cutils.bb_intersection_over_union(gt, b)>iou_thres:\n",
    "                                detected = True\n",
    "                            else:\n",
    "                                false_positives += 1\n",
    "                    if detected:\n",
    "                        true_positives += 1\n",
    "                    else:\n",
    "                        false_negatives += 1\n",
    "                overall_true_pos[k] += true_positives\n",
    "                overall_false_pos[k] += false_positives\n",
    "                overall_false_neg[k] += false_negatives\n",
    "                overall_boxes_count[k] += boxes_count\n",
    "        for k in np.arange(0.0, 1.01, 0.05):\n",
    "            if (overall_false_neg[k]+overall_true_pos[k])==0:\n",
    "                recalls[res].append(0.0)\n",
    "            else:\n",
    "                recalls[res].append(overall_true_pos[k]/(overall_false_neg[k]+overall_true_pos[k]))\n",
    "            if (overall_false_pos[k]+overall_true_pos[k])==0:\n",
    "                precision[res].append(0.0)\n",
    "            else:\n",
    "                precision[res].append(overall_true_pos[k]/(overall_false_pos[k]+overall_true_pos[k]))\n",
    "    return recalls, precision\n",
    "\n",
    "def get_ap_for_res(hparams, seed=2314134, split='test'):\n",
    "    recalls, precision = ap_model_seeds(hparams, seed, split)\n",
    "    \n",
    "    aps = dict()\n",
    "    for res in ['ges', 'geb', 'sie']:\n",
    "        prec = np.array(precisions[res])\n",
    "        rec = np.array(recalls[res])\n",
    "        ap = []\n",
    "        for t in np.arange(0.0, 1.01, 0.1):\n",
    "            prec_arr = prec[rec>t]\n",
    "            if len(prec_arr)==0:\n",
    "                ap.append(0.0)\n",
    "            else:\n",
    "                ap.append(prec_arr.max())\n",
    "        aps[res] = np.array(ap).mean()\n",
    "    return aps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
